{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# OpenFedLLM è”é‚¦å­¦ä¹ è®­ç»ƒ - Google Colabç‰ˆæœ¬\n",
    "\n",
    "æœ¬notebookç”¨äºåœ¨Google Colabä¸Šè¿è¡Œè”é‚¦å­¦ä¹ è®­ç»ƒï¼Œæ— éœ€GPT APIè¯„ä¼°ã€‚\n",
    "\n",
    "## ä½¿ç”¨è¯´æ˜\n",
    "1. åœ¨Colabä¸­è¿è¡Œæ—¶ï¼Œç¡®ä¿é€‰æ‹©GPUè¿è¡Œæ—¶ï¼ˆè¿è¡Œæ—¶ -> æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ -> GPUï¼‰\n",
    "2. æŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰cell\n",
    "3. è®­ç»ƒå®Œæˆåå¯ä»¥æŸ¥çœ‹è®­ç»ƒæŸå¤±æ›²çº¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "## æ­¥éª¤1: æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–åŒ…ï¼ˆæ™ºèƒ½å®‰è£…ï¼‰\n",
    "\n",
    "Colabå·²ç»é¢„è£…äº†å¾ˆå¤šåŒ…ï¼Œæˆ‘ä»¬åªå®‰è£…ç¼ºå¤±æˆ–ç‰ˆæœ¬ä¸åŒ¹é…çš„åŒ…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_packages"
   },
   "outputs": [],
   "source": [
    "# é¦–å…ˆæ£€æŸ¥å“ªäº›åŒ…å·²ç»å®‰è£…ï¼Œå“ªäº›éœ€è¦å®‰è£…\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# éœ€è¦æ£€æŸ¥çš„åŒ…åŠå…¶ç‰ˆæœ¬è¦æ±‚\n",
    "required_packages = {\n",
    "    'transformers': '4.31.0',\n",
    "    'peft': '0.4.0',\n",
    "    'trl': '0.7.2',\n",
    "    'bitsandbytes': '0.40.2',\n",
    "    'accelerate': '0.21.0',\n",
    "    'datasets': '2.13.0',\n",
    "    'torch': None,  # æ£€æŸ¥æ˜¯å¦å®‰è£…ï¼Œç‰ˆæœ¬ç”±Colabå†³å®š\n",
    "    'tqdm': None,  # é€šå¸¸å·²å®‰è£…\n",
    "    'numpy': None,  # é€šå¸¸å·²å®‰è£…\n",
    "}\n",
    "\n",
    "print(\"ğŸ” æ£€æŸ¥å·²å®‰è£…çš„åŒ…...\")\n",
    "packages_to_install = []\n",
    "\n",
    "for package, required_version in required_packages.items():\n",
    "    try:\n",
    "        module = importlib.import_module(package)\n",
    "        installed_version = getattr(module, '__version__', 'unknown')\n",
    "        \n",
    "        if required_version:\n",
    "            # æ£€æŸ¥ç‰ˆæœ¬æ˜¯å¦åŒ¹é…\n",
    "            if installed_version != required_version:\n",
    "                print(f\"  âš ï¸  {package}: {installed_version} (éœ€è¦ {required_version})\")\n",
    "                packages_to_install.append(f\"{package}=={required_version}\")\n",
    "            else:\n",
    "                print(f\"  âœ… {package}: {installed_version} (å·²å®‰è£…ï¼Œç‰ˆæœ¬æ­£ç¡®)\")\n",
    "        else:\n",
    "            print(f\"  âœ… {package}: {installed_version} (å·²å®‰è£…)\")\n",
    "    except ImportError:\n",
    "        print(f\"  âŒ {package}: æœªå®‰è£…\")\n",
    "        if required_version:\n",
    "            packages_to_install.append(f\"{package}=={required_version}\")\n",
    "        else:\n",
    "            packages_to_install.append(package)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if packages_to_install:\n",
    "    print(f\"éœ€è¦å®‰è£… {len(packages_to_install)} ä¸ªåŒ…:\")\n",
    "    for pkg in packages_to_install:\n",
    "        print(f\"  - {pkg}\")\n",
    "else:\n",
    "    print(\"âœ… æ‰€æœ‰å¿…éœ€çš„åŒ…éƒ½å·²å®‰è£…ä¸”ç‰ˆæœ¬æ­£ç¡®ï¼\")\n",
    "    print(\"å¯ä»¥è·³è¿‡å®‰è£…æ­¥éª¤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "# åªå®‰è£…ç¼ºå¤±æˆ–ç‰ˆæœ¬ä¸åŒ¹é…çš„åŒ…\n",
    "if packages_to_install:\n",
    "    print(f\"ğŸ“¦ æ­£åœ¨å®‰è£… {len(packages_to_install)} ä¸ªåŒ…...\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # æ‰¹é‡å®‰è£…ï¼ˆæ›´é«˜æ•ˆï¼‰\n",
    "    install_cmd = f\"pip install -q {' '.join(packages_to_install)}\"\n",
    "    print(f\"æ‰§è¡Œå‘½ä»¤: {install_cmd}\")\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        install_cmd.split(),\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nâœ… ä¾èµ–åŒ…å®‰è£…å®Œæˆï¼\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ å®‰è£…è¿‡ç¨‹ä¸­å¯èƒ½æœ‰è­¦å‘Šï¼Œä½†é€šå¸¸å¯ä»¥ç»§ç»­\")\n",
    "        if result.stderr:\n",
    "            print(f\"é”™è¯¯ä¿¡æ¯: {result.stderr[:500]}\")\n",
    "else:\n",
    "    print(\"âœ… æ‰€æœ‰åŒ…éƒ½å·²å®‰è£…ï¼Œè·³è¿‡å®‰è£…æ­¥éª¤\")\n",
    "\n",
    "# éªŒè¯å®‰è£…\n",
    "print(\"\\nğŸ” éªŒè¯å®‰è£…ç»“æœ...\")\n",
    "failed_packages = []\n",
    "for package in required_packages.keys():\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "        print(f\"  âœ… {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"  âŒ {package} å¯¼å…¥å¤±è´¥\")\n",
    "        failed_packages.append(package)\n",
    "\n",
    "if failed_packages:\n",
    "    print(f\"\\nâš ï¸ ä»¥ä¸‹åŒ…å¯¼å…¥å¤±è´¥: {failed_packages}\")\n",
    "    print(\"è¯·æ‰‹åŠ¨å®‰è£…: !pip install \" + \" \".join(failed_packages))\n",
    "else:\n",
    "    print(\"\\nâœ… æ‰€æœ‰åŒ…éªŒè¯é€šè¿‡ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone"
   },
   "source": [
    "## æ­¥éª¤2: å…‹éš†ä»£ç åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# å…‹éš†ä»“åº“ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰\n",
    "repo_url = 'https://github.com/GuangLun2000/OpenFedLLM-Attack.git'\n",
    "repo_name = 'OpenFedLLM-Attack'\n",
    "\n",
    "if not os.path.exists(repo_name):\n",
    "    print(f\"æ­£åœ¨å…‹éš†ä»“åº“: {repo_url}\")\n",
    "    !git clone --recursive --shallow-submodules {repo_url} {repo_name}\n",
    "    print(\"âœ… ä»£ç åº“å…‹éš†å®Œæˆï¼\")\n",
    "else:\n",
    "    print(\"âœ… ä»£ç åº“å·²å­˜åœ¨ï¼Œè·³è¿‡å…‹éš†\")\n",
    "\n",
    "# è¿›å…¥é¡¹ç›®ç›®å½•\n",
    "os.chdir(repo_name)\n",
    "print(f\"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}\")\n",
    "\n",
    "# éªŒè¯å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "required_files = ['main_sft.py', 'config.py', 'utils', 'federated_learning']\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"  âœ… {file} å­˜åœ¨\")\n",
    "    else:\n",
    "        print(f\"  âŒ {file} ä¸å­˜åœ¨ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## æ­¥éª¤3: è®¾ç½®ç¯å¢ƒå˜é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_env"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# è®¾ç½®PYTHONPATH\n",
    "fingpt_dir = 'evaluation/FinGPT'\n",
    "if os.path.exists(fingpt_dir):\n",
    "    sys.path.insert(0, os.path.abspath(fingpt_dir))\n",
    "\n",
    "# å°†å½“å‰ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆï¼\")\n",
    "print(f\"Pythonè·¯å¾„: {sys.path[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "debug"
   },
   "source": [
    "## æ­¥éª¤4: è°ƒè¯•å’ŒéªŒè¯ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "debug_env"
   },
   "outputs": [],
   "source": [
    "# éªŒè¯æ‰€æœ‰å¿…è¦çš„æ¨¡å—æ˜¯å¦å¯ä»¥å¯¼å…¥\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"ğŸ” ç¯å¢ƒæ£€æŸ¥:\")\n",
    "print(\"\\n1. æ£€æŸ¥Pythonè·¯å¾„:\")\n",
    "print(f\"   å½“å‰ç›®å½•: {os.getcwd()}\")\n",
    "print(f\"   Pythonè·¯å¾„åŒ…å«: {os.getcwd() in sys.path}\")\n",
    "\n",
    "print(\"\\n2. æ£€æŸ¥å…³é”®æ–‡ä»¶:\")\n",
    "key_files = ['main_sft.py', 'config.py', 'utils/__init__.py', 'federated_learning/__init__.py']\n",
    "for file in key_files:\n",
    "    exists = os.path.exists(file)\n",
    "    status = \"âœ…\" if exists else \"âŒ\"\n",
    "    print(f\"   {status} {file}\")\n",
    "\n",
    "print(\"\\n3. å°è¯•å¯¼å…¥å…³é”®æ¨¡å—:\")\n",
    "try:\n",
    "    from utils import *\n",
    "    print(\"   âœ… utils æ¨¡å—å¯¼å…¥æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ utils æ¨¡å—å¯¼å…¥å¤±è´¥: {e}\")\n",
    "\n",
    "try:\n",
    "    from federated_learning import *\n",
    "    print(\"   âœ… federated_learning æ¨¡å—å¯¼å…¥æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ federated_learning æ¨¡å—å¯¼å…¥å¤±è´¥: {e}\")\n",
    "\n",
    "try:\n",
    "    from config import get_config\n",
    "    print(\"   âœ… config æ¨¡å—å¯¼å…¥æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ config æ¨¡å—å¯¼å…¥å¤±è´¥: {e}\")\n",
    "\n",
    "print(\"\\n4. æ£€æŸ¥ä¾èµ–åŒ…:\")\n",
    "packages = ['transformers', 'peft', 'trl', 'bitsandbytes', 'accelerate', 'datasets', 'torch']\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "        print(f\"   âœ… {pkg}\")\n",
    "    except ImportError:\n",
    "        print(f\"   âŒ {pkg} æœªå®‰è£…\")\n",
    "\n",
    "print(\"\\nâœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_gpu"
   },
   "source": [
    "## æ­¥éª¤5: æ£€æŸ¥GPUå’ŒCUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu_cuda"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "    print(f\"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œè¯·ç¡®ä¿åœ¨Colabä¸­é€‰æ‹©äº†GPUè¿è¡Œæ—¶ï¼\")\n",
    "    print(\"   è®¾ç½®æ–¹æ³•: è¿è¡Œæ—¶ -> æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ -> ç¡¬ä»¶åŠ é€Ÿå™¨ -> GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## æ­¥éª¤6: é…ç½®è®­ç»ƒå‚æ•°ï¼ˆé€‚åˆColabçš„å°è§„æ¨¡é…ç½®ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_config"
   },
   "outputs": [],
   "source": [
    "# ========== è®­ç»ƒå‚æ•°é…ç½® ==========\n",
    "# è¿™äº›å‚æ•°å·²ç»ä¼˜åŒ–ä¸ºé€‚åˆColabçš„èµ„æºé™åˆ¶\n",
    "\n",
    "training_config = {\n",
    "    # æ¨¡å‹é…ç½®\n",
    "    'model_name_or_path': 'meta-llama/Llama-2-7b-hf',  # åŸºç¡€æ¨¡å‹\n",
    "    'use_peft': True,  # ä½¿ç”¨LoRAï¼ˆå¿…é¡»ï¼‰\n",
    "    'peft_lora_r': 16,  # LoRA rankï¼ˆé™ä½ä»¥èŠ‚çœæ˜¾å­˜ï¼‰\n",
    "    'peft_lora_alpha': 32,  # LoRA alpha\n",
    "    'load_in_8bit': True,  # 8bité‡åŒ–ï¼ˆå¿…é¡»ï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰\n",
    "    \n",
    "    # æ•°æ®é›†é…ç½®\n",
    "    'dataset_name': 'vicgalle/alpaca-gpt4',  # æ•°æ®é›†\n",
    "    'dataset_sample': 5000,  # ä½¿ç”¨5000ä¸ªæ ·æœ¬ï¼ˆå‡å°‘æ•°æ®é‡ï¼‰\n",
    "    'template': 'alpaca',  # æ¨¡æ¿\n",
    "    \n",
    "    # è”é‚¦å­¦ä¹ é…ç½®\n",
    "    'fed_alg': 'fedavg',  # è”é‚¦å­¦ä¹ ç®—æ³•\n",
    "    'num_clients': 5,  # å®¢æˆ·ç«¯æ•°é‡ï¼ˆå‡å°‘ï¼‰\n",
    "    'sample_clients': 2,  # æ¯è½®é‡‡æ ·å®¢æˆ·ç«¯æ•°\n",
    "    'num_rounds': 10,  # è®­ç»ƒè½®æ•°ï¼ˆå‡å°‘ç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰\n",
    "    \n",
    "    # è®­ç»ƒé…ç½®\n",
    "    'max_steps': 5,  # æ¯è½®æ¯ä¸ªå®¢æˆ·ç«¯çš„è®­ç»ƒæ­¥æ•°ï¼ˆå‡å°‘ï¼‰\n",
    "    'batch_size': 4,  # æ‰¹æ¬¡å¤§å°ï¼ˆé™ä½ä»¥èŠ‚çœæ˜¾å­˜ï¼‰\n",
    "    'gradient_accumulation_steps': 4,  # æ¢¯åº¦ç´¯ç§¯ï¼ˆä¿æŒæœ‰æ•ˆbatch sizeï¼‰\n",
    "    'learning_rate': 2e-5,  # å­¦ä¹ ç‡\n",
    "    'seq_length': 256,  # åºåˆ—é•¿åº¦ï¼ˆé™ä½ä»¥èŠ‚çœæ˜¾å­˜ï¼‰\n",
    "    \n",
    "    # è¾“å‡ºé…ç½®\n",
    "    'output_dir': './output_colab',  # è¾“å‡ºç›®å½•\n",
    "    'save_model_freq': 5,  # æ¯5è½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "    'logging_steps': 10,  # æ—¥å¿—æ­¥æ•°\n",
    "    'seed': 2023,  # éšæœºç§å­\n",
    "}\n",
    "\n",
    "print(\"âœ… è®­ç»ƒå‚æ•°é…ç½®å®Œæˆï¼\")\n",
    "print(\"\\né…ç½®æ‘˜è¦:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prepare_args"
   },
   "source": [
    "## æ­¥éª¤7: å‡†å¤‡å‘½ä»¤è¡Œå‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_arguments"
   },
   "outputs": [],
   "source": [
    "# æ„å»ºå‘½ä»¤è¡Œå‚æ•°åˆ—è¡¨\n",
    "args_list = ['main_sft.py']  # è„šæœ¬åç§°\n",
    "\n",
    "# æ·»åŠ æ‰€æœ‰å‚æ•°\n",
    "for key, value in training_config.items():\n",
    "    if isinstance(value, bool):\n",
    "        if value:\n",
    "            args_list.append(f\"--{key}\")\n",
    "    else:\n",
    "        args_list.append(f\"--{key}\")\n",
    "        args_list.append(str(value))\n",
    "\n",
    "# æ·»åŠ è”é‚¦å­¦ä¹ ç‰¹å®šå‚æ•°ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰\n",
    "if '--save_model_freq' not in args_list:\n",
    "    args_list.extend([\n",
    "        '--save_model_freq', str(training_config['save_model_freq']),\n",
    "    ])\n",
    "\n",
    "print(\"âœ… å‘½ä»¤è¡Œå‚æ•°å‡†å¤‡å®Œæˆï¼\")\n",
    "print(f\"\\nå°†æ‰§è¡Œçš„å‘½ä»¤:\")\n",
    "print(f\"python {' '.join(args_list)}\")\n",
    "print(f\"\\nå‚æ•°æ•°é‡: {len(args_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train"
   },
   "source": [
    "## æ­¥éª¤8: å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [],
   "source": [
    "# æ„å»ºå®Œæ•´çš„å‘½ä»¤å­—ç¬¦ä¸²\n",
    "cmd_args = args_list[1:]  # ç§»é™¤è„šæœ¬åç§°\n",
    "cmd_str = ' '.join([str(arg) for arg in cmd_args])\n",
    "\n",
    "print(\"ğŸš€ å¼€å§‹è”é‚¦å­¦ä¹ è®­ç»ƒ...\")\n",
    "print(\"â° é¢„è®¡æ—¶é—´: æ ¹æ®GPUæ€§èƒ½ï¼Œå¯èƒ½éœ€è¦30åˆ†é’Ÿåˆ°2å°æ—¶\")\n",
    "print(f\"\\næ‰§è¡Œå‘½ä»¤: python main_sft.py {cmd_str[:100]}...\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nè¯·åœ¨ä¸‹é¢çš„cellä¸­æ‰§è¡Œè®­ç»ƒå‘½ä»¤ï¼ˆä½¿ç”¨!pythonï¼‰\")\n",
    "print(\"\\næˆ–è€…å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„ä»£ç è‡ªåŠ¨æ‰§è¡Œ:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training_exec"
   },
   "outputs": [],
   "source": [
    "# æ‰§è¡Œè®­ç»ƒå‘½ä»¤\n",
    "# æ³¨æ„ï¼šåœ¨Colabä¸­ï¼Œä½¿ç”¨!å‘½ä»¤æ‰§è¡ŒPythonè„šæœ¬æ˜¯æœ€å¯é çš„æ–¹æ³•\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•\n",
    "if not os.path.exists('main_sft.py'):\n",
    "    print(\"âŒ é”™è¯¯: æ‰¾ä¸åˆ° main_sft.pyï¼Œè¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•\")\n",
    "    print(f\"å½“å‰ç›®å½•: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"æ­£åœ¨æ‰§è¡Œè®­ç»ƒ...\")\n",
    "    print(f\"å·¥ä½œç›®å½•: {os.getcwd()}\")\n",
    "    print(f\"Pythonè·¯å¾„: {sys.executable}\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "    try:\n",
    "        # ä½¿ç”¨subprocessæ‰§è¡Œï¼Œæ•è·è¾“å‡º\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, 'main_sft.py'] + cmd_args,\n",
    "            check=False,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        # æ˜¾ç¤ºè¾“å‡º\n",
    "        if result.stdout:\n",
    "            print(\"æ ‡å‡†è¾“å‡º:\")\n",
    "            print(result.stdout)\n",
    "        \n",
    "        if result.stderr:\n",
    "            print(\"\\né”™è¯¯è¾“å‡º:\")\n",
    "            print(result.stderr)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… è®­ç»ƒå®Œæˆï¼\")\n",
    "        else:\n",
    "            print(f\"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¿”å›ç : {result.returncode}\")\n",
    "            print(\"\\nğŸ’¡ è°ƒè¯•å»ºè®®:\")\n",
    "            print(\"  1. æ£€æŸ¥ä¸Šé¢çš„é”™è¯¯è¾“å‡º\")\n",
    "            print(\"  2. ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²å®‰è£…\")\n",
    "            print(\"  3. æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨\")\n",
    "            print(\"  4. å°è¯•ä½¿ç”¨ä¸‹é¢çš„ç›´æ¥æ‰§è¡Œæ–¹æ³•\")\n",
    "            print(\"\\nğŸ’¡ æˆ–è€…æ‰‹åŠ¨æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤:\")\n",
    "            print(f\"!python main_sft.py {cmd_str}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ‰§è¡Œå‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nğŸ’¡ è¯·æ‰‹åŠ¨æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤:\")\n",
    "        print(f\"!python main_sft.py {cmd_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_training_direct"
   },
   "source": [
    "## æ­¥éª¤7-å¤‡é€‰: ç›´æ¥æ‰§è¡Œå‘½ä»¤ï¼ˆå¦‚æœä¸Šé¢çš„æ–¹æ³•å¤±è´¥ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_direct_command"
   },
   "outputs": [],
   "source": [
    "# æ–¹æ³•2: ç›´æ¥ä½¿ç”¨!å‘½ä»¤æ‰§è¡Œï¼ˆæ¨èï¼Œæ›´å¯é ï¼‰\n",
    "# å¦‚æœä¸Šé¢çš„subprocessæ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨è¿™ä¸ªæ–¹æ³•\n",
    "\n",
    "print(\"ğŸ’¡ å¦‚æœä¸Šé¢çš„æ–¹æ³•å¤±è´¥ï¼Œè¯·ä½¿ç”¨ä¸‹é¢çš„ç›´æ¥æ‰§è¡Œæ–¹æ³•:\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"æ‰§è¡Œå‘½ä»¤ï¼ˆå¤åˆ¶åˆ°æ–°çš„cellä¸­è¿è¡Œï¼‰:\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"!python main_sft.py {cmd_str}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\næˆ–è€…ç›´æ¥è¿è¡Œä¸‹é¢çš„ä»£ç :\")\n",
    "print(\"\\nï¼ˆå–æ¶ˆæ³¨é‡Šä¸‹é¢çš„ä»£ç ï¼‰\")\n",
    "\n",
    "# å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šæ¥ç›´æ¥æ‰§è¡Œ\n",
    "# !python main_sft.py {cmd_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_direct_exec"
   },
   "outputs": [],
   "source": [
    "# æ–¹æ³•2: ç›´æ¥ä½¿ç”¨!å‘½ä»¤æ‰§è¡Œï¼ˆæœ€å¯é çš„æ–¹æ³•ï¼‰\n",
    "# åœ¨Colabä¸­ï¼Œç›´æ¥ä½¿ç”¨!å‘½ä»¤æ‰§è¡Œæ˜¯æœ€å¯é çš„æ–¹æ³•\n",
    "\n",
    "import os\n",
    "\n",
    "# ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•\n",
    "if os.path.exists('main_sft.py'):\n",
    "    print(\"ğŸš€ å‡†å¤‡æ‰§è¡Œè®­ç»ƒå‘½ä»¤...\")\n",
    "    print(f\"å·¥ä½œç›®å½•: {os.getcwd()}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"è¯·å¤åˆ¶ä¸‹é¢çš„å‘½ä»¤åˆ°æ–°çš„cellä¸­æ‰§è¡Œ:\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"!python main_sft.py {cmd_str}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\\næˆ–è€…ç›´æ¥åœ¨è¿™ä¸ªcellä¸­æ‰§è¡Œï¼ˆå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šï¼‰:\")\n",
    "    print(\"\\næ³¨æ„: åœ¨Jupyter/Colabä¸­ï¼Œ!å‘½ä»¤éœ€è¦åœ¨å•ç‹¬çš„cellä¸­æ‰§è¡Œ\")\n",
    "    \n",
    "    # ä¸‹é¢çš„ä»£ç ä¼šåœ¨æ‰§è¡Œæ—¶æ˜¾ç¤ºå‘½ä»¤ï¼Œä½†å®é™…æ‰§è¡Œéœ€è¦ç”¨æˆ·æ‰‹åŠ¨åˆ›å»ºæ–°cell\n",
    "    # æˆ–è€…ä½¿ç”¨get_ipython().run_cell_magic()\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        ipython = get_ipython()\n",
    "        if ipython:\n",
    "            print(\"\\nâœ… æ£€æµ‹åˆ°IPythonç¯å¢ƒï¼Œå¯ä»¥ç›´æ¥æ‰§è¡Œ\")\n",
    "            print(\"å–æ¶ˆä¸‹é¢ä»£ç çš„æ³¨é‡Šæ¥æ‰§è¡Œ:\")\n",
    "            # å–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œæ¥æ‰§è¡Œ\n",
    "            # ipython.run_cell_magic('bash', '', f'python main_sft.py {cmd_str}')\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"âŒ é”™è¯¯: æ‰¾ä¸åˆ° main_sft.py\")\n",
    "    print(f\"å½“å‰ç›®å½•: {os.getcwd()}\")\n",
    "    print(\"è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize"
   },
   "source": [
    "## æ­¥éª¤8: å¯è§†åŒ–è®­ç»ƒæŸå¤±ï¼ˆæ— éœ€APIï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_loss"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# åŠ è½½è®­ç»ƒæŸå¤±\n",
    "loss_file = os.path.join(training_config['output_dir'], 'training_loss.npy')\n",
    "\n",
    "if os.path.exists(loss_file):\n",
    "    training_loss = np.load(loss_file)\n",
    "    \n",
    "    print(f\"è®­ç»ƒæŸå¤±å½¢çŠ¶: {training_loss.shape}\")\n",
    "    print(f\"å®¢æˆ·ç«¯æ•°é‡: {training_loss.shape[0]}\")\n",
    "    print(f\"è®­ç»ƒè½®æ•°: {training_loss.shape[1]}\")\n",
    "    \n",
    "    # ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # ç»˜åˆ¶æ¯ä¸ªå®¢æˆ·ç«¯çš„æŸå¤±\n",
    "    for client_id in range(training_loss.shape[0]):\n",
    "        client_loss = training_loss[client_id]\n",
    "        # è¿‡æ»¤æ‰-1ï¼ˆæœªå‚ä¸è®­ç»ƒçš„è½®æ¬¡ï¼‰\n",
    "        valid_rounds = np.where(client_loss >= 0)[0]\n",
    "        if len(valid_rounds) > 0:\n",
    "            plt.plot(valid_rounds + 1, client_loss[valid_rounds], \n",
    "                    alpha=0.6, label=f'Client {client_id}', linewidth=1.5)\n",
    "    \n",
    "    # è®¡ç®—å¹¶ç»˜åˆ¶å¹³å‡æŸå¤±\n",
    "    avg_loss = []\n",
    "    for round_id in range(training_loss.shape[1]):\n",
    "        round_losses = training_loss[:, round_id]\n",
    "        valid_losses = round_losses[round_losses >= 0]\n",
    "        if len(valid_losses) > 0:\n",
    "            avg_loss.append(np.mean(valid_losses))\n",
    "        else:\n",
    "            avg_loss.append(np.nan)\n",
    "    \n",
    "    valid_avg_rounds = [i+1 for i, v in enumerate(avg_loss) if not np.isnan(v)]\n",
    "    valid_avg_loss = [v for v in avg_loss if not np.isnan(v)]\n",
    "    \n",
    "    if len(valid_avg_rounds) > 0:\n",
    "        plt.plot(valid_avg_rounds, valid_avg_loss, \n",
    "                'k-', linewidth=3, label='Average Loss', marker='o', markersize=6)\n",
    "    \n",
    "    plt.xlabel('Training Round', fontsize=12)\n",
    "    plt.ylabel('Training Loss', fontsize=12)\n",
    "    plt.title('Federated Learning Training Loss', fontsize=14, fontweight='bold')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯\n",
    "    print(\"\\nğŸ“Š è®­ç»ƒç»Ÿè®¡ä¿¡æ¯:\")\n",
    "    print(f\"  åˆå§‹å¹³å‡æŸå¤±: {valid_avg_loss[0]:.4f}\")\n",
    "    print(f\"  æœ€ç»ˆå¹³å‡æŸå¤±: {valid_avg_loss[-1]:.4f}\")\n",
    "    print(f\"  æŸå¤±ä¸‹é™: {valid_avg_loss[0] - valid_avg_loss[-1]:.4f}\")\n",
    "    print(f\"  æŸå¤±ä¸‹é™ç™¾åˆ†æ¯”: {(valid_avg_loss[0] - valid_avg_loss[-1]) / valid_avg_loss[0] * 100:.2f}%\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ æœªæ‰¾åˆ°è®­ç»ƒæŸå¤±æ–‡ä»¶: {loss_file}\")\n",
    "    print(\"   è¯·ç¡®ä¿è®­ç»ƒå·²å®Œæˆå¹¶ä¿å­˜äº†æŸå¤±æ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_output"
   },
   "source": [
    "## æ­¥éª¤10: æ£€æŸ¥è¾“å‡ºæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_files"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "output_dir = training_config['output_dir']\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    print(f\"âœ… è¾“å‡ºç›®å½•å­˜åœ¨: {output_dir}\")\n",
    "    print(\"\\nğŸ“ è¾“å‡ºæ–‡ä»¶åˆ—è¡¨:\")\n",
    "    \n",
    "    for item in os.listdir(output_dir):\n",
    "        item_path = os.path.join(output_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  ğŸ“‚ {item}/ (ç›®å½•)\")\n",
    "        else:\n",
    "            size = os.path.getsize(item_path)\n",
    "            if size < 1024:\n",
    "                size_str = f\"{size} B\"\n",
    "            elif size < 1024 * 1024:\n",
    "                size_str = f\"{size / 1024:.2f} KB\"\n",
    "            else:\n",
    "                size_str = f\"{size / (1024 * 1024):.2f} MB\"\n",
    "            print(f\"  ğŸ“„ {item} ({size_str})\")\n",
    "    \n",
    "    # è¯»å–å¹¶æ˜¾ç¤ºé…ç½®\n",
    "    config_file = os.path.join(output_dir, 'args.json')\n",
    "    if os.path.exists(config_file):\n",
    "        print(\"\\nğŸ“‹ è®­ç»ƒé…ç½®:\")\n",
    "        with open(config_file, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        print(json.dumps(config, indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(f\"âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tips"
   },
   "source": [
    "## ğŸ’¡ ä½¿ç”¨æç¤º\n",
    "\n",
    "### å¦‚æœé‡åˆ°é—®é¢˜ï¼š\n",
    "\n",
    "1. **æ˜¾å­˜ä¸è¶³ (OOM)**ï¼š\n",
    "   - è¿›ä¸€æ­¥å‡å° `batch_size` (æ”¹ä¸º2æˆ–1)\n",
    "   - å‡å° `seq_length` (æ”¹ä¸º128)\n",
    "   - å‡å° `peft_lora_r` (æ”¹ä¸º8)\n",
    "   - å‡å° `dataset_sample` (æ”¹ä¸º2000)\n",
    "\n",
    "2. **è®­ç»ƒå¤ªæ…¢**ï¼š\n",
    "   - å‡å°‘ `num_rounds` (æ”¹ä¸º5)\n",
    "   - å‡å°‘ `max_steps` (æ”¹ä¸º3)\n",
    "   - å‡å°‘ `dataset_sample` (æ”¹ä¸º2000)\n",
    "\n",
    "3. **æ¨¡å‹ä¸‹è½½å¤±è´¥**ï¼š\n",
    "   - ç¡®ä¿å·²ç™»å½•HuggingFace: `huggingface-cli login`\n",
    "   - æˆ–è€…ä½¿ç”¨æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹è·¯å¾„\n",
    "\n",
    "4. **æƒ³è¦æ›´å®Œæ•´çš„è®­ç»ƒ**ï¼š\n",
    "   - å¢åŠ  `num_rounds` (æ”¹ä¸º20-50)\n",
    "   - å¢åŠ  `max_steps` (æ”¹ä¸º10)\n",
    "   - å¢åŠ  `dataset_sample` (æ”¹ä¸º10000)\n",
    "   - å¢åŠ  `num_clients` (æ”¹ä¸º10)\n",
    "\n",
    "### ä¸‹ä¸€æ­¥ï¼š\n",
    "- è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹ä¿å­˜åœ¨ `output_colab/checkpoint-*` ç›®å½•ä¸­\n",
    "- å¯ä»¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†\n",
    "- å¦‚éœ€è¯„ä¼°ï¼Œå¯ä»¥è¿è¡Œè¯„ä¼°è„šæœ¬ï¼ˆéœ€è¦API keyï¼‰"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
